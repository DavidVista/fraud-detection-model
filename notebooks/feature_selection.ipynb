{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf15daa",
   "metadata": {},
   "source": [
    "# Feature Selection Using Genetic Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aff1e7",
   "metadata": {},
   "source": [
    "#### In this notebook, we present the implementation of the feature selection using GA, the results of the search, and the list of features to be used for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63f9f66",
   "metadata": {},
   "source": [
    "Start with the existing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1544839",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from deap import base, creator, tools, algorithms\n",
    "from pyswarms.single import GlobalBestPSO\n",
    "from functools import partial\n",
    "from typing import Dict, List, Tuple, Union, Optional\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "class ModelPipeline:\n",
    "    \"\"\"\n",
    "    A flexible, modular pipeline for feature selection,\n",
    "    models tuning, and training.\n",
    "\n",
    "    Attributes:\n",
    "        data_path (str): path to the dataset (Pandas parquet)\n",
    "        sample_frac (float): part of a dataset used for training\n",
    "        X (np.array): feature vectors\n",
    "        Y (np.array): labels\n",
    "        selected_features (List[str]): list of selected features\n",
    "        scaler ()\n",
    "        X_train, X_test, y_train, y_test (np.array's): train and test dataset splits\n",
    "        models (List[Dict]): list of dictionaries describing models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, selected_features=None, sample_frac: float = 1.0):\n",
    "        \"\"\"Initialize the pipeline with data loading\"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.sample_frac = sample_frac\n",
    "        self.selected_features = selected_features\n",
    "        self._load_data()\n",
    "        self.scaler = None\n",
    "        self.models = {\n",
    "            'RandomForest': {\n",
    "                'tune_func': self.tune_random_forest,\n",
    "                'train_func': self.train_random_forest,\n",
    "                'params': None\n",
    "            },\n",
    "            'XGBoost': {\n",
    "                'tune_func': self.tune_xgboost,\n",
    "                'train_func': self.train_xgboost,\n",
    "                'params': None\n",
    "            },\n",
    "            'LightGBM': {\n",
    "                'tune_func': self.tune_lightgbm,\n",
    "                'train_func': self.train_lightgbm,\n",
    "                'params': None\n",
    "            },\n",
    "            'DenseNN': {\n",
    "                'tune_func': self.tune_dense_nn,\n",
    "                'train_func': self.train_dense_nn,\n",
    "                'params': None\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _random_undersample(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"Undersample dataset for binary labels\"\"\"\n",
    "        a_label, b_label = y.unique()\n",
    "        a_label_cnt, b_label_cnt = y.value_counts()\n",
    "        minority_label = a_label if (a_label_cnt < b_label_cnt) else b_label\n",
    "        majority_label = b_label if (a_label_cnt < b_label_cnt) else a_label\n",
    "        minority_cnt = a_label_cnt if (a_label_cnt < b_label_cnt) else b_label_cnt\n",
    "\n",
    "        # Select all elements with minority label\n",
    "        minority_y = y[y == minority_label]\n",
    "        new_X = X.loc[minority_y.index]\n",
    "        new_y = minority_y\n",
    "\n",
    "        # Sample elements with majority label\n",
    "        majority_y = y[y == majority_label]\n",
    "        sampled_majority = majority_y.sample(n=minority_cnt, random_state=42)\n",
    "        new_X = pd.concat([new_X, X.loc[sampled_majority.index]])\n",
    "        new_y = pd.concat([new_y, sampled_majority])\n",
    "\n",
    "        return new_X, new_y\n",
    "\n",
    "    def _load_data(self) -> None:\n",
    "        \"\"\"Load and prepare data\"\"\"\n",
    "        X = pd.read_parquet(self.data_path)\n",
    "        y = X['remainder__isFraud']\n",
    "        X = X.drop(columns=['remainder__isFraud'])\n",
    "\n",
    "        \"\"\"Prepare train/test split and optionally select features\"\"\"\n",
    "        if self.selected_features is not None:\n",
    "            X = X.iloc[:, self.selected_features]\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        # Data split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        # Undersampling for equal proportion of labels\n",
    "        self.X_train, self.y_train = self._random_undersample(X_train, y_train)\n",
    "        self.X_test, self.y_test = X_test, y_test\n",
    "\n",
    "        # Take a sample using sample_frac\n",
    "        if self.sample_frac < 1.0:\n",
    "            self.X_train = self.X_train.sample(frac=self.sample_frac, random_state=42)\n",
    "            self.y_train = self.y_train.loc[self.X_train.index]\n",
    "\n",
    "    def evaluate_features(self, individual, X_train, y_train, X_val, y_val) -> Tuple[float]:\n",
    "        \"\"\"Evaluate fitness of feature subset using RandomForest\"\"\"\n",
    "        selected = [i for i, val in enumerate(individual) if val == 1]\n",
    "\n",
    "        if not selected:  # At least one feature must be selected\n",
    "            return 0.0,\n",
    "\n",
    "        X_train_sub = X_train.iloc[:, selected]\n",
    "        X_val_sub = X_val.iloc[:, selected]\n",
    "\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=50,\n",
    "            max_depth=5,\n",
    "            class_weight='balanced_subsample',\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_train_sub, y_train)\n",
    "        y_proba = model.predict_proba(X_val_sub)[:, 1]\n",
    "        score = roc_auc_score(y_val, y_proba)\n",
    "\n",
    "        # Add penalty for too many features\n",
    "        feature_penalty = len(selected) / X_train.shape[1] * 0.1\n",
    "        return score - feature_penalty,\n",
    "\n",
    "    def genetic_feature_selection(self, n_pop: int = 50, n_gen: int = 20,\n",
    "                                  cxpb: float = 0.5, mutpb: float = 0.2) -> List[int]:\n",
    "        \"\"\"Perform feature selection using Genetic Algorithm\"\"\"\n",
    "        # Split data for feature selection\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            self.X, self.y, test_size=0.2, stratify=self.y, random_state=42\n",
    "        )\n",
    "\n",
    "        # DEAP setup\n",
    "        creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "        creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "        toolbox = base.Toolbox()\n",
    "        toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "        toolbox.register(\"individual\", tools.initRepeat, creator.Individual,\n",
    "                         toolbox.attr_bool, n=self.X.shape[1])\n",
    "        toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "        eval_fn = partial(self.evaluate_features,\n",
    "                          X_train=X_train, y_train=y_train,\n",
    "                          X_val=X_val, y_val=y_val)\n",
    "        toolbox.register(\"evaluate\", eval_fn)\n",
    "        toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "        toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "        toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "        # Run GA\n",
    "        pop = toolbox.population(n=n_pop)\n",
    "        hof = tools.HallOfFame(1)\n",
    "        stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "        stats.register(\"avg\", np.mean)\n",
    "        stats.register(\"min\", np.min)\n",
    "        stats.register(\"max\", np.max)\n",
    "\n",
    "        pop, log = algorithms.eaSimple(\n",
    "            pop, toolbox, cxpb=cxpb, mutpb=mutpb, ngen=n_gen,\n",
    "            stats=stats, halloffame=hof, verbose=True\n",
    "        )\n",
    "\n",
    "        best_individual = hof[0]\n",
    "        selected_features = [i for i, val in enumerate(best_individual) if val == 1]\n",
    "\n",
    "        print(f\"\\nSelected {len(selected_features)} features out of {self.X.shape[1]}\")\n",
    "        print(\"Selected feature indices:\", selected_features)\n",
    "\n",
    "        self.selected_features = selected_features\n",
    "        return selected_features\n",
    "\n",
    "    # ==================== Random Forest ====================\n",
    "    def tune_random_forest(self, n_particles: int = 5, iters: int = 10) -> Dict[str, float]:\n",
    "        \"\"\"PSO optimization for Random Forest hyperparameters\"\"\"\n",
    "\n",
    "        bounds = (\n",
    "            np.array([50, 2, 2, 1]),    # min values\n",
    "            np.array([500, 30, 10, 30])  # max values\n",
    "        )\n",
    "\n",
    "        def objective_function(params):\n",
    "            scores = []\n",
    "            for param_set in params:\n",
    "                n_estimators = int(param_set[0])\n",
    "                max_depth = int(param_set[1])\n",
    "                min_samples_split = int(param_set[2])\n",
    "                max_features = min(int(param_set[3]), self.X_train.shape[1])\n",
    "\n",
    "                model = RandomForestClassifier(\n",
    "                    n_estimators=n_estimators,\n",
    "                    max_depth=max_depth,\n",
    "                    min_samples_split=min_samples_split,\n",
    "                    max_features=max_features,\n",
    "                    class_weight='balanced_subsample',\n",
    "                    n_jobs=-1,\n",
    "                    random_state=42\n",
    "                )\n",
    "\n",
    "                cv_scores = []\n",
    "                cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "                for train_idx, val_idx in cv.split(self.X_train, self.y_train):\n",
    "                    X_train_cv, X_val_cv = self.X_train.iloc[train_idx], self.X_train.iloc[val_idx]\n",
    "                    y_train_cv, y_val_cv = self.y_train.iloc[train_idx], self.y_train.iloc[val_idx]\n",
    "\n",
    "                    model.fit(X_train_cv, y_train_cv)\n",
    "                    y_proba = model.predict_proba(X_val_cv)[:, 1]\n",
    "                    score = roc_auc_score(y_val_cv, y_proba)\n",
    "                    cv_scores.append(score)\n",
    "\n",
    "                scores.append(-np.mean(cv_scores))\n",
    "\n",
    "            return np.array(scores)\n",
    "\n",
    "        optimizer = GlobalBestPSO(\n",
    "            n_particles=n_particles,\n",
    "            dimensions=len(bounds[0]),\n",
    "            options={'c1': 0.5, 'c2': 0.3, 'w': 0.9, 'early_stop': True, 'patience': 3},\n",
    "            bounds=bounds\n",
    "        )\n",
    "\n",
    "        best_cost, best_params = optimizer.optimize(objective_function, iters=iters)\n",
    "\n",
    "        optimized_params = {\n",
    "            'n_estimators': int(best_params[0]),\n",
    "            'max_depth': int(best_params[1]),\n",
    "            'min_samples_split': int(best_params[2]),\n",
    "            'max_features': int(best_params[3])\n",
    "        }\n",
    "\n",
    "        print(\"\\nOptimized Random Forest Parameters:\")\n",
    "        print(optimized_params)\n",
    "        print(f\"Best ROC-AUC: {-best_cost:.4f}\")\n",
    "\n",
    "        self.models['RandomForest']['params'] = optimized_params\n",
    "        return optimized_params\n",
    "\n",
    "    def train_random_forest(self) -> Dict[str, Union[float, str]]:\n",
    "        \"\"\"Train Random Forest with optimized parameters\"\"\"\n",
    "        if not self.models['RandomForest']['params']:\n",
    "            raise ValueError(\"Random Forest parameters not tuned. Call tune_random_forest() first.\")\n",
    "\n",
    "        params = self.models['RandomForest']['params']\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=params['n_estimators'],\n",
    "            max_depth=params['max_depth'],\n",
    "            min_samples_split=params['min_samples_split'],\n",
    "            max_features=params['max_features'],\n",
    "            class_weight='balanced_subsample',\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        model.fit(self.X_train, self.y_train)\n",
    "        return self._evaluate_model(model, \"Random Forest\")\n",
    "\n",
    "    # ==================== XGBoost ====================\n",
    "    def tune_xgboost(self, n_particles: int = 5, iters: int = 10) -> Dict[str, float]:\n",
    "        \"\"\"PSO optimization for XGBoost hyperparameters\"\"\"\n",
    "\n",
    "        bounds = (\n",
    "            np.array([0.01, 3, 0.1, 0.1, 0.1, 0]),  # min values\n",
    "            np.array([0.3, 10, 10, 1, 1, 5])        # max values\n",
    "        )\n",
    "\n",
    "        def objective_function(params):\n",
    "            scores = []\n",
    "            for param_set in params:\n",
    "                model = XGBClassifier(\n",
    "                    learning_rate=param_set[0],\n",
    "                    max_depth=int(param_set[1]),\n",
    "                    min_child_weight=param_set[2],\n",
    "                    subsample=param_set[3],\n",
    "                    colsample_bytree=param_set[4],\n",
    "                    gamma=param_set[5],\n",
    "                    scale_pos_weight=np.sqrt(len(self.y_train)/self.y_train.sum()),\n",
    "                    tree_method='hist',\n",
    "                    eval_metric='aucpr',\n",
    "                    random_state=42\n",
    "                )\n",
    "\n",
    "                cv_scores = []\n",
    "                cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "                for train_idx, val_idx in cv.split(self.X_train, self.y_train):\n",
    "                    X_train_cv, X_val_cv = self.X_train.iloc[train_idx], self.X_train.iloc[val_idx]\n",
    "                    y_train_cv, y_val_cv = self.y_train.iloc[train_idx], self.y_train.iloc[val_idx]\n",
    "\n",
    "                    model.fit(X_train_cv, y_train_cv)\n",
    "                    y_proba = model.predict_proba(X_val_cv)[:, 1]\n",
    "                    score = roc_auc_score(y_val_cv, y_proba)\n",
    "                    cv_scores.append(score)\n",
    "\n",
    "                scores.append(-np.mean(cv_scores))\n",
    "\n",
    "            return np.array(scores)\n",
    "\n",
    "        optimizer = GlobalBestPSO(\n",
    "            n_particles=n_particles,\n",
    "            dimensions=len(bounds[0]),\n",
    "            options={'c1': 0.5, 'c2': 0.3, 'w': 0.9, 'early_stop': True, 'patience': 3},\n",
    "            bounds=bounds\n",
    "        )\n",
    "\n",
    "        best_cost, best_params = optimizer.optimize(objective_function, iters=iters)\n",
    "\n",
    "        optimized_params = {\n",
    "            'learning_rate': best_params[0],\n",
    "            'max_depth': int(best_params[1]),\n",
    "            'min_child_weight': best_params[2],\n",
    "            'subsample': best_params[3],\n",
    "            'colsample_bytree': best_params[4],\n",
    "            'gamma': best_params[5]\n",
    "        }\n",
    "\n",
    "        print(\"\\nOptimized XGBoost Parameters:\")\n",
    "        print(optimized_params)\n",
    "        print(f\"Best ROC-AUC: {-best_cost:.4f}\")\n",
    "\n",
    "        self.models['XGBoost']['params'] = optimized_params\n",
    "        return optimized_params\n",
    "\n",
    "    def train_xgboost(self) -> Dict[str, Union[float, str]]:\n",
    "        \"\"\"Train XGBoost with optimized parameters\"\"\"\n",
    "        if not self.models['XGBoost']['params']:\n",
    "            raise ValueError(\"XGBoost parameters not tuned. Call tune_xgboost() first.\")\n",
    "\n",
    "        params = self.models['XGBoost']['params']\n",
    "        model = XGBClassifier(\n",
    "            learning_rate=params['learning_rate'],\n",
    "            max_depth=params['max_depth'],\n",
    "            min_child_weight=params['min_child_weight'],\n",
    "            subsample=params['subsample'],\n",
    "            colsample_bytree=params['colsample_bytree'],\n",
    "            gamma=params['gamma'],\n",
    "            scale_pos_weight=np.sqrt(len(self.y_train)/self.y_train.sum()),\n",
    "            tree_method='hist',\n",
    "            eval_metric='aucpr',\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        model.fit(self.X_train, self.y_train)\n",
    "        return self._evaluate_model(model, \"XGBoost\")\n",
    "\n",
    "    # ==================== LightGBM ====================\n",
    "    def tune_lightgbm(self, n_particles: int = 5, iters: int = 10) -> Dict[str, float]:\n",
    "        \"\"\"PSO optimization for LightGBM hyperparameters\"\"\"\n",
    "\n",
    "        bounds = (\n",
    "            np.array([0.01, 3, 0.1, 0.1, 0.1, 20]),  # min values\n",
    "            np.array([0.3, 50, 100, 1, 1, 50])       # max values\n",
    "        )\n",
    "\n",
    "        def objective_function(params):\n",
    "            scores = []\n",
    "            for param_set in params:\n",
    "                model = LGBMClassifier(\n",
    "                    learning_rate=param_set[0],\n",
    "                    num_leaves=int(param_set[1]),\n",
    "                    min_data_in_leaf=int(param_set[2]),\n",
    "                    feature_fraction=param_set[3],\n",
    "                    bagging_fraction=param_set[4],\n",
    "                    bagging_freq=int(param_set[5]),\n",
    "                    class_weight='balanced',\n",
    "                    boosting_type='gbdt',\n",
    "                    objective='binary',\n",
    "                    random_state=42\n",
    "                )\n",
    "\n",
    "                cv_scores = []\n",
    "                cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "                for train_idx, val_idx in cv.split(self.X_train, self.y_train):\n",
    "                    X_train_cv, X_val_cv = self.X_train.iloc[train_idx], self.X_train.iloc[val_idx]\n",
    "                    y_train_cv, y_val_cv = self.y_train.iloc[train_idx], self.y_train.iloc[val_idx]\n",
    "\n",
    "                    model.fit(X_train_cv, y_train_cv)\n",
    "                    y_proba = model.predict_proba(X_val_cv)[:, 1]\n",
    "                    score = roc_auc_score(y_val_cv, y_proba)\n",
    "                    cv_scores.append(score)\n",
    "\n",
    "                scores.append(-np.mean(cv_scores))\n",
    "\n",
    "            return np.array(scores)\n",
    "\n",
    "        optimizer = GlobalBestPSO(\n",
    "            n_particles=n_particles,\n",
    "            dimensions=len(bounds[0]),\n",
    "            options={'c1': 0.5, 'c2': 0.3, 'w': 0.9, 'early_stop': True, 'patience': 3},\n",
    "            bounds=bounds\n",
    "        )\n",
    "\n",
    "        best_cost, best_params = optimizer.optimize(objective_function, iters=iters)\n",
    "\n",
    "        optimized_params = {\n",
    "            'learning_rate': best_params[0],\n",
    "            'num_leaves': int(best_params[1]),\n",
    "            'min_data_in_leaf': int(best_params[2]),\n",
    "            'feature_fraction': best_params[3],\n",
    "            'bagging_fraction': best_params[4],\n",
    "            'bagging_freq': int(best_params[5])\n",
    "        }\n",
    "\n",
    "        print(\"\\nOptimized LightGBM Parameters:\")\n",
    "        print(optimized_params)\n",
    "        print(f\"Best ROC-AUC: {-best_cost:.4f}\")\n",
    "\n",
    "        self.models['LightGBM']['params'] = optimized_params\n",
    "        return optimized_params\n",
    "\n",
    "    def train_lightgbm(self) -> Dict[str, Union[float, str]]:\n",
    "        \"\"\"Train LightGBM with optimized parameters\"\"\"\n",
    "        if not self.models['LightGBM']['params']:\n",
    "            raise ValueError(\"LightGBM parameters not tuned. Call tune_lightgbm() first.\")\n",
    "\n",
    "        params = self.models['LightGBM']['params']\n",
    "        model = LGBMClassifier(\n",
    "            learning_rate=params['learning_rate'],\n",
    "            num_leaves=params['num_leaves'],\n",
    "            min_data_in_leaf=params['min_data_in_leaf'],\n",
    "            feature_fraction=params['feature_fraction'],\n",
    "            bagging_fraction=params['bagging_fraction'],\n",
    "            bagging_freq=params['bagging_freq'],\n",
    "            class_weight='balanced',\n",
    "            boosting_type='gbdt',\n",
    "            objective='binary',\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        model.fit(self.X_train, self.y_train)\n",
    "        return self._evaluate_model(model, \"LightGBM\")\n",
    "\n",
    "    # ==================== Dense Neural Network ====================\n",
    "    def _create_dense_nn(self, input_dim: int, layers: Tuple[int, ...] = (64, 32),\n",
    "                         learning_rate: float = 0.001, dropout_rate: float = 0.2) -> Sequential:\n",
    "        \"\"\"Create a dense neural network architecture\"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Input((input_dim,)))\n",
    "        model.add(Dense(layers[0], activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "        for units in layers[1:]:\n",
    "            model.add(Dense(units, activation='relu'))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=learning_rate),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def tune_dense_nn(self, n_particles: int = 5, iters: int = 10) -> Dict[str, float]:\n",
    "        \"\"\"PSO optimization for Dense NN hyperparameters\"\"\"\n",
    "\n",
    "        # Scale data for NN\n",
    "        self.scaler = StandardScaler()\n",
    "        X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
    "\n",
    "        bounds = (\n",
    "            np.array([0.0001, 16, 16, 0.1, 0.1]),  # min values\n",
    "            np.array([0.01, 256, 256, 0.5, 0.5])   # max values\n",
    "        )\n",
    "\n",
    "        def objective_function(params):\n",
    "            scores = []\n",
    "            for param_set in params:\n",
    "                model = KerasClassifier(\n",
    "                    model=lambda: self._create_dense_nn(\n",
    "                        input_dim=X_train_scaled.shape[1],\n",
    "                        layers=(int(param_set[1]), int(param_set[2])),\n",
    "                        learning_rate=param_set[0],\n",
    "                        dropout_rate=param_set[4]\n",
    "                    ),\n",
    "                    epochs=10,\n",
    "                    batch_size=256,\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                cv_scores = []\n",
    "                cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "                for train_idx, val_idx in cv.split(X_train_scaled, self.y_train):\n",
    "                    X_train_cv, X_val_cv = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "                    y_train_cv, y_val_cv = self.y_train.iloc[train_idx], self.y_train.iloc[val_idx]\n",
    "\n",
    "                    model.fit(X_train_cv, y_train_cv)\n",
    "                    y_proba = model.predict_proba(X_val_cv)[:, 1]\n",
    "                    score = roc_auc_score(y_val_cv, y_proba)\n",
    "                    cv_scores.append(score)\n",
    "\n",
    "                scores.append(-np.mean(cv_scores))\n",
    "\n",
    "            return np.array(scores)\n",
    "\n",
    "        optimizer = GlobalBestPSO(\n",
    "            n_particles=n_particles,\n",
    "            dimensions=len(bounds[0]),\n",
    "            options={'c1': 0.5, 'c2': 0.3, 'w': 0.9, 'early_stop': True, 'patience': 3},\n",
    "            bounds=bounds\n",
    "        )\n",
    "\n",
    "        best_cost, best_params = optimizer.optimize(objective_function, iters=iters)\n",
    "\n",
    "        optimized_params = {\n",
    "            'learning_rate': best_params[0],\n",
    "            'layer1': int(best_params[1]),\n",
    "            'layer2': int(best_params[2]),\n",
    "            'dropout_rate': best_params[4]\n",
    "        }\n",
    "\n",
    "        print(\"\\nOptimized Dense NN Parameters:\")\n",
    "        print(optimized_params)\n",
    "        print(f\"Best ROC-AUC: {-best_cost:.4f}\")\n",
    "\n",
    "        self.models['DenseNN']['params'] = optimized_params\n",
    "        return optimized_params\n",
    "\n",
    "    def train_dense_nn(self) -> Dict[str, Union[float, str]]:\n",
    "        \"\"\"Train Dense NN with optimized parameters\"\"\"\n",
    "        if not self.models['DenseNN']['params']:\n",
    "            raise ValueError(\"Dense NN parameters not tuned. Call tune_dense_nn() first.\")\n",
    "\n",
    "        params = self.models['DenseNN']['params']\n",
    "\n",
    "        # Scale data\n",
    "        X_train_scaled = self.scaler.transform(self.X_train)\n",
    "\n",
    "        model = KerasClassifier(\n",
    "            model=lambda: self._create_dense_nn(\n",
    "                input_dim=X_train_scaled.shape[1],\n",
    "                layers=(params['layer1'], params['layer2']),\n",
    "                learning_rate=params['learning_rate'],\n",
    "                dropout_rate=params['dropout_rate']\n",
    "            ),\n",
    "            epochs=50,\n",
    "            batch_size=256,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        model.fit(X_train_scaled, self.y_train)\n",
    "        return self._evaluate_model(model, \"Dense Neural Network\", is_nn=True)\n",
    "\n",
    "    # ==================== Evaluation ====================\n",
    "    def _evaluate_model(self, model, model_name: str, is_nn: bool = False,\n",
    "                        X_test: Optional[np.ndarray] = None) -> Dict[str, Union[float, str]]:\n",
    "        \"\"\"Evaluate model performance and create visualizations\"\"\"\n",
    "        if X_test is None:\n",
    "            X_test = self.X_test\n",
    "\n",
    "        if is_nn:\n",
    "            y_proba = model.predict_proba(X_test)[:, 1]\n",
    "            y_pred = (y_proba > 0.5).astype(int)\n",
    "        else:\n",
    "            y_proba = model.predict_proba(X_test)[:, 1]\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "        # Metrics calculation\n",
    "        metrics = {\n",
    "            \"roc_auc\": roc_auc_score(self.y_test, y_proba),\n",
    "            \"pr_auc\": average_precision_score(self.y_test, y_proba),\n",
    "            \"classification_report\": classification_report(self.y_test, y_pred)\n",
    "        }\n",
    "\n",
    "        if not is_nn and hasattr(model, 'feature_importances_'):\n",
    "            metrics[\"feature_importance\"] = model.feature_importances_\n",
    "\n",
    "        # Visualization\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        ConfusionMatrixDisplay.from_predictions(self.y_test, y_pred, ax=ax[0])\n",
    "        ax[0].set_title(f\"Confusion Matrix - {model_name}\")\n",
    "\n",
    "        if not is_nn and hasattr(model, 'feature_importances_'):\n",
    "            # Top 10 most valuable features\n",
    "            top_features = self.X.columns[\n",
    "                np.argsort(metrics.get('feature_importance', np.zeros(self.X.shape[1])))[-10:]\n",
    "            ]\n",
    "            ax[1].barh(top_features, metrics.get('feature_importance', np.zeros(self.X.shape[1]))[-10:])\n",
    "            ax[1].set_title(f\"Top 10 Features - {model_name}\")\n",
    "        else:\n",
    "            ax[1].axis('off')\n",
    "\n",
    "        fig.tight_layout()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def run_pipeline(self, models_to_run: List[str] = None,\n",
    "                     selected_features: List[int] = None) -> Dict[str, Dict[str, Union[float, str]]]:\n",
    "        \"\"\"\n",
    "        Run the complete pipeline with optional model selection\n",
    "\n",
    "        Args:\n",
    "            models_to_run: List of model names to run (None for all)\n",
    "            feature_selection: Whether to perform feature selection\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of results for each model\n",
    "        \"\"\"\n",
    "        if models_to_run is None:\n",
    "            models_to_run = list(self.models.keys())\n",
    "\n",
    "        # Perform feature selection if requested\n",
    "        if selected_features is None:\n",
    "            self.selected_features = list(range(self.X.shape[1]))\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        for model_name in models_to_run:\n",
    "            if model_name not in self.models:\n",
    "                print(f\"Warning: Model {model_name} not found in available models. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\n=== Processing {model_name} ===\")\n",
    "\n",
    "            # Tune hyperparameters\n",
    "            print(f\"Tuning {model_name} hyperparameters...\")\n",
    "            self.models[model_name]['tune_func']()\n",
    "\n",
    "            # Train model with optimized parameters\n",
    "            print(f\"Training {model_name} with optimized parameters...\")\n",
    "            results[model_name] = self.models[model_name]['train_func']()\n",
    "\n",
    "            # Print results\n",
    "            print(f\"\\n{model_name} Results:\")\n",
    "            print(f\"ROC-AUC: {results[model_name]['roc_auc']:.4f}\")\n",
    "            print(f\"PR-AUC: {results[model_name]['pr_auc']:.4f}\")\n",
    "            print(\"Classification Report:\")\n",
    "            print(results[model_name]['classification_report'])\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f649999c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = ModelPipeline(\n",
    "    data_path='encoded_fraud_data.parquet',\n",
    "    sample_frac=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7095b1a",
   "metadata": {},
   "source": [
    "Initiate GA for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9d060e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Starting Genetic Algorithm for feature selection...\")\n",
    "features = pipeline.genetic_feature_selection()\n",
    "print(features)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
